{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "import llama_index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Document, VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import QueryPipeline, InputComponent, Link, FunctionComponent\n",
    "\n",
    "from scraper.scraper import start_scraping\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data\n",
    "log_filename = f\"scrapping.log\"\n",
    "start_url = \"https://manuals.sma.de/STPxx50/en-US/index.html\"\n",
    "url_prefix = \"https://manuals.sma.de/STPxx50/en-US\"\n",
    "max_depth = 5\n",
    "num_workers = 100\n",
    "\n",
    "scraped_data = start_scraping(\n",
    "    start_url=start_url,\n",
    "    url_prefix=url_prefix,\n",
    "    max_depth=max_depth,\n",
    "    num_workers=num_workers,\n",
    "    log_filename=log_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init llms\n",
    "llm_model = OpenAI(model='gpt-4o-mini')\n",
    "emb_model = OpenAIEmbedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest data\n",
    "documents = [Document(text=d['text'], extra_info={'url':d['url'], 'path':d['path']}) for d in scraped_data]\n",
    "pipeline = IngestionPipeline(transformations=[emb_model])\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init index\n",
    "index = VectorStoreIndex(nodes=nodes, embed_model=emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init retriever\n",
    "retriever = VectorIndexRetriever(index, similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init response synthesizer\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "QA_PROMPT = PromptTemplate(QA_PROMPT_TMPL)\n",
    "\n",
    "REFINE_PROMPT_TMPL = (\n",
    "    \"The original query is as follows: {query_str}\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. \"\n",
    "    \"If the context isn't useful, return the original answer.\\n\"\n",
    "    \"Always try to keep the answer concise and relevant to the query.\\n\"\n",
    "    \"There is no need to include information beyond the scope of the query.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "REFINE_PROMPT = PromptTemplate(REFINE_PROMPT_TMPL)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=llm_model,\n",
    "    response_mode='refine',\n",
    "    text_qa_template=QA_PROMPT,\n",
    "    refine_template=REFINE_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init query engine\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query(query):\n",
    "    WRAP_LEN = 200\n",
    "    _wrap = lambda x: textwrap.fill(x, WRAP_LEN, replace_whitespace=False)\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    print(f\"Query: {_wrap(query)}\")\n",
    "    print(f\"Response:\")\n",
    "    print(_wrap(response.response))\n",
    "\n",
    "    print('\\n\\nsources')\n",
    "    for i, node in enumerate(sorted(response.source_nodes, key=lambda x: x.score)):\n",
    "        print(f\"Node {i}\")\n",
    "        print(f\"URL: {node.metadata['url']}\")\n",
    "        print(f\"Path: {' > '.join(node.metadata['path'])}\")\n",
    "        # print(textwrap.fill(node.text, WRAP_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = send_query(\"what are the different output power classes of the inverter? For each what is the mpp voltage range? what is the euro-efficiency of the inverter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query pipeline\n",
    "\n",
    "def output_formatter(llm_response, source_nodes):\n",
    "    WRAP_LEN = 200\n",
    "    _wrap = lambda x: textwrap.fill(x, WRAP_LEN, replace_whitespace=False)\n",
    "\n",
    "    out = (\n",
    "        f\"Response:\\n\"\n",
    "        f\"{_wrap(llm_response.response)}\"\n",
    "        \"\\n\\nsources\"\n",
    "    )\n",
    "\n",
    "    for i, node in enumerate(sorted(source_nodes, key=lambda x: x.score)):\n",
    "        out +=f\"\\nNode {i}\"\n",
    "        out +=f\"\\nURL: {node.metadata['url']}\"\n",
    "        out +=f\"\\nPath: {' > '.join(node.metadata['path'])}\"\n",
    "\n",
    "    return out\n",
    "\n",
    "rag_2 = QueryPipeline(verbose=True)\n",
    "\n",
    "rag_2.add_modules(\n",
    "    module_dict={\n",
    "        'input': InputComponent(),\n",
    "        'retriever': retriever,\n",
    "        'response_synthesizer': response_synthesizer.as_query_component(),\n",
    "        'output_formatter': FunctionComponent(output_formatter),\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_2.add_links(\n",
    "    [\n",
    "        Link('input', 'retriever'),\n",
    "        Link('retriever', 'response_synthesizer', dest_key='nodes'),\n",
    "        Link('retriever', 'output_formatter', dest_key='source_nodes'),\n",
    "        Link('input', 'response_synthesizer', dest_key='query_str'),\n",
    "        Link('response_synthesizer', 'output_formatter', dest_key='llm_response'),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rag_2.run(question=\"what area the different output power classes of the inverter? For each what is the mpp voltage range? what is the euro-efficiency of the inverter?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
